{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG with Postgres flexible server\n",
    "\n",
    "You can run this notebook after running succesfully the \"RAG - Postgresql - create embeddings\" notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML, JSON, Markdown\n",
    "import os\n",
    "\n",
    "# Configure environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure OpenAI API\n",
    "OPENAI_GPT35_DEPLOYMENT_NAME = os.getenv(\"OPENAI_GPT35_DEPLOYMENT_NAME\")\n",
    "OPENAI_GPT4_DEPLOYMENT_NAME = os.getenv(\"OPENAI_GPT4_DEPLOYMENT_NAME\")\n",
    "OPENAI_GPT4V_DEPLOYMENT_NAME = os.getenv(\"OPENAI_GPT4V_DEPLOYMENT_NAME\")\n",
    "OPENAI_ADA_EMBEDDING_DEPLOYMENT_NAME = os.getenv(\"OPENAI_ADA_EMBEDDING_DEPLOYMENT_NAME\")\n",
    "OPENAI_DALLE_DEPLOYMENT_NAME = os.getenv(\"OPENAI_DALLE_DEPLOYMENT_NAME\")\n",
    "\n",
    "OPENAI_DEPLOYMENT_ENDPOINT = os.getenv(\"OPENAI_DEPLOYMENT_ENDPOINT\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# configure postgres\n",
    "POSTGRES_USER = os.getenv(\"POSTGRES_USER\")\n",
    "POSTGRES_PASSWORD = os.getenv(\"POSTGRES_PASSWORD\")\n",
    "POSTGRES_HOST = os.getenv(\"POSTGRES_HOST\")\n",
    "POSTGRES_DB = os.getenv(\"POSTGRES_DB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddingmodel = AzureOpenAIEmbeddings(\n",
    "    deployment=OPENAI_ADA_EMBEDDING_DEPLOYMENT_NAME,\n",
    "    model=OPENAI_ADA_EMBEDDING_DEPLOYMENT_NAME,\n",
    "    azure_endpoint=OPENAI_DEPLOYMENT_ENDPOINT,\n",
    "    chunk_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we use the tenacity library to create delays and retries when calling openAI embeddings to avoid hitting throttling limits\n",
    "@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6))\n",
    "def calc_embeddings(text):\n",
    "    deployment = OPENAI_ADA_EMBEDDING_DEPLOYMENT_NAME\n",
    "    # replace newlines, which can negatively affect performance.\n",
    "    txt = text.replace(\"\\n\", \" \")\n",
    "    return embeddingmodel.embed_query(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection established\n"
     ]
    }
   ],
   "source": [
    "# connect to postgres\n",
    "import psycopg2\n",
    "\n",
    "sslmode = \"require\"\n",
    "conn_string = \"host={0} user={1} dbname={2} password={3} sslmode={4}\".format(POSTGRES_HOST, POSTGRES_USER, POSTGRES_DB, POSTGRES_PASSWORD, sslmode)\n",
    "conn = psycopg2.connect(conn_string) \n",
    "print(\"Connection established\")\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('VECTOR',)\n"
     ]
    }
   ],
   "source": [
    "# Checking the vector extension exists\n",
    "show_extensions_query = \"SHOW azure.extensions;\"\n",
    "cursor.execute(show_extensions_query)\n",
    "conn.commit()\n",
    "results = cursor.fetchall()\n",
    "for row in results:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drawn from it, and then had the iron part placed in the coffin along  \n",
      "with one of the paddles of his boat. All by his own request, also,  \n",
      "biscuits were then ranged round the sides within: a flask of fresh  \n",
      "water was placed at the head, and a small bag of woody earth scraped up  \n",
      "in the hold at the foot; and a piece of sail -cloth being rolled up for  \n",
      "a pillow, Queequeg now entreated to be lifted into his final bed, that  \n",
      "he might make trial of its comforts, if any it had. He lay without  \n",
      "moving a few minutes, then told one to go to his bag and bring out his  \n",
      "little god, Yojo. Then crossing his arms on his breast with Yojo  \n",
      "between, he called for the coffin lid (hatch he called it) to be placed  \n",
      "over him. The head part turned over with a leather hinge, and there lay  \n",
      "Queequeg in his coffin with little but his composed countenance in  \n",
      "view. “Rarmai” (it will do; it is easy), he murmured at last, and  \n",
      "signed to be replaced in his hammock.\n"
     ]
    }
   ],
   "source": [
    "from pgvector.psycopg2 import register_vector\n",
    "from psycopg2 import Error\n",
    "from psycopg2 import sql\n",
    "import numpy as np\n",
    "\n",
    "# Register 'pgvector' type for the 'embedding' column\n",
    "register_vector(conn)\n",
    "table_name = \"embeddings\"\n",
    "\n",
    "# perform a similarity search between a query and the ingested documents\n",
    "question = \"Why does the coffin prepared for Queequeg become Ishmael's life buoy once the Pequod sinks?\"\n",
    "retrieve_k = 1 # for retrieving the top k reviews from the database\n",
    "# Generate embeddings for the question and retrieve the top k document chunks\n",
    "questionEmbedding = calc_embeddings(question)\n",
    "\n",
    "\n",
    "select_query = f\"SELECT content FROM {table_name} ORDER BY embedding <-> %s LIMIT {retrieve_k}\"\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(select_query, (np.array(questionEmbedding),))\n",
    "results = cursor.fetchall()\n",
    "\n",
    "answer = results[0][0]\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "clientOpenAI = AzureOpenAI(\n",
    "  azure_endpoint = OPENAI_DEPLOYMENT_ENDPOINT, \n",
    "  api_key=OPENAI_API_KEY,  \n",
    "  api_version=\"2023-05-15\"\n",
    ")\n",
    "\n",
    "def call_openAI(text):\n",
    "    response = clientOpenAI.chat.completions.create(\n",
    "        model=OPENAI_GPT35_DEPLOYMENT_NAME,\n",
    "        messages = text,\n",
    "        temperature=0.7,\n",
    "        max_tokens=800,\n",
    "        top_p=0.95,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        stop=None\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "The coffin prepared for Queequeg becomes Ishmael's life buoy once the Pequod sinks because he had the iron part of the coffin placed in it, along with one of the paddles of his boat. Additionally, biscuits were placed inside the coffin, along with a flask of fresh water, a small bag of woody earth, and a rolled-up piece of sail-cloth for a pillow. Queequeg requested to be lifted into his final bed to test its comfort. After a few minutes, he asked for his little god, Yojo, to be brought out from his bag. Queequeg then asked for the coffin lid, or \"hatch\" as he called it, to be placed over him. He lay in his coffin with his composed countenance in view, and murmured \"Rarmai\" (it will do; it is easy) before requesting to be placed back in his hammock."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = 'Question: {}'.format(question) + '\\n' + 'Information: {}'.format(answer)\n",
    "# prepare prompt\n",
    "messages = [{\"role\": \"system\", \"content\": \"You are a HELPFUL assistant answering users questions. Answer the question using the provided information and do not add anything else.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "result = call_openAI(messages)\n",
    "display(HTML(result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
