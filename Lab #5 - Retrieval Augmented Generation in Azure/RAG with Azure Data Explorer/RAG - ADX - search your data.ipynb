{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG with Azure Data Explorer  \n",
    "\n",
    "You can run this notebook after running succesfully the \"RAG - ADX - create embeddings\" notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML, JSON, Markdown\n",
    "import os\n",
    "\n",
    "# Configure environment variables\n",
    "load_dotenv()\n",
    "\n",
    "AAD_TENANT_ID = os.getenv(\"AAD_TENANT_ID\")\n",
    "KUSTO_CLUSTER = os.getenv(\"KUSTO_CLUSTER\")\n",
    "KUSTO_DATABASE = os.getenv(\"KUSTO_DATABASE\")\n",
    "KUSTO_TABLE = os.getenv(\"KUSTO_TABLE\")\n",
    "KUSTO_MANAGED_IDENTITY_APP_ID = os.getenv(\"KUSTO_MANAGED_IDENTITY_APP_ID\")\n",
    "KUSTO_MANAGED_IDENTITY_SECRET = os.getenv(\"KUSTO_MANAGED_IDENTITY_SECRET\")\n",
    "\n",
    "# Configure OpenAI API\n",
    "OPENAI_GPT35_DEPLOYMENT_NAME = os.getenv(\"OPENAI_GPT35_DEPLOYMENT_NAME\")\n",
    "OPENAI_GPT4_DEPLOYMENT_NAME = os.getenv(\"OPENAI_GPT4_DEPLOYMENT_NAME\")\n",
    "OPENAI_GPT4V_DEPLOYMENT_NAME = os.getenv(\"OPENAI_GPT4V_DEPLOYMENT_NAME\")\n",
    "OPENAI_ADA_EMBEDDING_DEPLOYMENT_NAME = os.getenv(\"OPENAI_ADA_EMBEDDING_DEPLOYMENT_NAME\")\n",
    "OPENAI_DALLE_DEPLOYMENT_NAME = os.getenv(\"OPENAI_DALLE_DEPLOYMENT_NAME\")\n",
    "\n",
    "OPENAI_DEPLOYMENT_ENDPOINT = os.getenv(\"OPENAI_DEPLOYMENT_ENDPOINT\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.kusto.data import KustoClient, KustoConnectionStringBuilder\n",
    "from azure.kusto.data.exceptions import KustoServiceError\n",
    "from azure.kusto.data.helpers import dataframe_from_result_table\n",
    "\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use GPT to answer a question based on the question and the answer from our similarity search\n",
    "from openai import AzureOpenAI\n",
    "clientOpenAI = AzureOpenAI(\n",
    "  azure_endpoint = OPENAI_DEPLOYMENT_ENDPOINT, \n",
    "  api_key=OPENAI_API_KEY,  \n",
    "  api_version=\"2023-05-15\"\n",
    ")\n",
    "\n",
    "def call_openAI(text):\n",
    "    response = clientOpenAI.chat.completions.create(\n",
    "        model=OPENAI_GPT35_DEPLOYMENT_NAME,\n",
    "        messages = text,\n",
    "        temperature=0.7,\n",
    "        max_tokens=800,\n",
    "        top_p=0.95,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        stop=None\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddingmodel = AzureOpenAIEmbeddings(\n",
    "    deployment=OPENAI_ADA_EMBEDDING_DEPLOYMENT_NAME,\n",
    "    model=OPENAI_ADA_EMBEDDING_DEPLOYMENT_NAME,\n",
    "    azure_endpoint=OPENAI_DEPLOYMENT_ENDPOINT,\n",
    "    chunk_size = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to adx using AAD app registration\n",
    "cluster = KUSTO_CLUSTER\n",
    "kcsb = KustoConnectionStringBuilder.with_aad_application_key_authentication(cluster, KUSTO_MANAGED_IDENTITY_APP_ID, KUSTO_MANAGED_IDENTITY_SECRET,  AAD_TENANT_ID)\n",
    "client = KustoClient(kcsb)\n",
    "kusto_db = KUSTO_DATABASE\n",
    "table_name = \"embeddings\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing the connection to kusto works - sample query to get the top 10 results from wikipedia\n",
    "query = table_name + \" | take 2\"\n",
    "\n",
    "response = client.execute(kusto_db, query)\n",
    "for row in response.primary_results[0]:\n",
    "    txt = (row[\"content\"])[0:10]\n",
    "    print(\"Title :{}\".format(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we use the tenacity library to create delays and retries when calling openAI embeddings to avoid hitting throttling limits\n",
    "@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6))\n",
    "def calc_embeddings(text):\n",
    "    deployment = OPENAI_ADA_EMBEDDING_DEPLOYMENT_NAME\n",
    "    # replace newlines, which can negatively affect performance.\n",
    "    txt = text.replace(\"\\n\", \" \")\n",
    "    return embeddingmodel.embed_query(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer_from_adx(question, nr_of_answers=1):\n",
    "        searchedEmbedding = calc_embeddings(question)\n",
    "        kusto_query = table_name + \" | extend similarity = series_cosine_similarity_fl(dynamic(\"+str(searchedEmbedding)+\"), embedding,1,1) | top \" + str(nr_of_answers) + \" by similarity desc \"\n",
    "        response = client.execute(kusto_db, kusto_query)\n",
    "\n",
    "        for row in response.primary_results[0]:\n",
    "                return row['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the question we want to ask and its embeddings\n",
    "question = \"Why does the coffin prepared for Queequeg become Ishmael's life buoy once the Pequod sinks?\"\n",
    "answer_from_ADX = get_answer_from_adx(\"Why does the coffin prepared for Queequeg become Ishmael's life buoy once the Pequod sinks?\",1)\n",
    "print(answer_from_ADX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Question: {}'.format(question) + '\\n' + 'Information: {}'.format(answer_from_ADX)\n",
    "# prepare prompt\n",
    "messages = [{\"role\": \"system\", \"content\": \"You are a HELPFUL assistant answering users questions. Answer the question using the provided information and do not add anything else.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "result = call_openAI(messages)\n",
    "display(HTML(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Why does Ahab pursue Moby Dick?\"\n",
    "retrieved_answer_from_adx = get_answer_from_adx(question,1)\n",
    "\n",
    "prompt = 'Question: {}'.format(question) + '\\n' + 'Information: {}'.format(retrieved_answer_from_adx)\n",
    "\n",
    "# prepare prompt\n",
    "messages = [{\"role\": \"system\", \"content\": \"You are a HELPFUL assistant answering users questions. Answer the question using the provided information and do not add anything else.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "result = call_openAI(messages)\n",
    "display(HTML(result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
