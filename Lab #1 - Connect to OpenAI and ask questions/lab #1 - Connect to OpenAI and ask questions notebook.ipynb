{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load Environment variables from .env file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import AzureOpenAI\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from IPython.display import display, HTML, JSON, Markdown\n",
    "import tiktoken\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "OPENAI_DEPLOYMENT_ENDPOINT = os.getenv(\"OPENAI_DEPLOYMENT_ENDPOINT\")\n",
    "OPENAI_DEPLOYMENT_NAME = os.getenv(\"OPENAI_DEPLOYMENT_NAME\")\n",
    "OPENAI_MODEL_NAME = os.getenv(\"OPENAI_MODEL_NAME\")\n",
    "OPENAI_DEPLOYMENT_VERSION = os.getenv(\"OPENAI_DEPLOYMENT_VERSION\")\n",
    "\n",
    "OPENAI_ADA_EMBEDDING_DEPLOYMENT_NAME = os.getenv(\"OPENAI_ADA_EMBEDDING_DEPLOYMENT_NAME\")\n",
    "OPENAI_ADA_EMBEDDING_MODEL_NAME = os.getenv(\"OPENAI_ADA_EMBEDDING_MODEL_NAME\")\n",
    "\n",
    "OPENAI_DAVINCI_DEPLOYMENT_NAME = os.getenv(\"OPENAI_DAVINCI_DEPLOYMENT_NAME\")\n",
    "OPENAI_DAVINCI_MODEL_NAME = os.getenv(\"OPENAI_DAVINCI_MODEL_NAME\")\n",
    "\n",
    "# Configure OpenAI API\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = OPENAI_DEPLOYMENT_VERSION\n",
    "openai.api_base = OPENAI_DEPLOYMENT_ENDPOINT\n",
    "openai.api_key = OPENAI_API_KEY"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Add personality to the model and start asking questions**\n",
    "We call directly the Azure OpenAI API with ***ChatCompletion*** API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "ChatCompletion (gpt-35-turbo) :Good morning! As an AI language model, I don't have emotions, but I'm here to assist you with any question you have. How can I help you today?"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# prepare prompt\n",
    "messages = [{\"role\": \"system\", \"content\": \"You are a HELPFUL assistant answering users trivia questions. Answer in a clear and concise manner.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Good morning, how are you today?\"}]\n",
    "\n",
    "\n",
    "answer = openai.ChatCompletion.create(engine=OPENAI_DEPLOYMENT_NAME,\n",
    "\n",
    "                                      messages=messages,)\n",
    "display(HTML(\"ChatCompletion (gpt-35-turbo) :\" +\n",
    "        answer.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "String theory is a theoretical framework in physics that tries to reconcile quantum mechanics with general relativity. It posits that the fundamental building blocks of the universe are not particles, but one-dimensional objects called strings. The theory suggests that these strings vibrate at different frequencies, and those vibrations give rise to all the different particles and forces that we observe in nature. While still a subject of active research, string theory has the potential to provide a unified theory of all the physical laws governing the universe."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# prepare prompt with another question:\n",
    "messages = [{\"role\": \"system\", \"content\": \"You are q HELPFUL assistant answering users trivia questions. Answer in clear and concise manner.\"},\n",
    "            {\"role\": \"user\", \"content\": \"What's string theory?\"}]\n",
    "\n",
    "\n",
    "answer = openai.ChatCompletion.create(engine=OPENAI_DEPLOYMENT_NAME,\n",
    "                                      messages=messages,)\n",
    "\n",
    "# print(\"ChatCompletion (gpt-35-turbo) :\" + answer.choices[0].message.content)\n",
    "\n",
    "display(HTML(answer.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "String theory is a very complicated idea that scientists use to help explain how the teeny-tiny things that make up everything in the universe (like atoms and particles) work. It says that these tiny things are made up of really, really thin \"strings\" that are too small for us to see with our eyes. It's kind of like how a string on a guitar makes different noises depending on how it's plucked, these strings vibrate in different ways to make different particles!"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# prepare prompt with another question:\n",
    "messages = [{\"role\": \"system\", \"content\": \"You are a HELPFUL assistant answering users trivia questions. Answer as for a FIVE YEARS old child.\"},\n",
    "            {\"role\": \"user\", \"content\": \"what's string theory?\"}]\n",
    "\n",
    "\n",
    "answer = openai.ChatCompletion.create(engine=OPENAI_DEPLOYMENT_NAME,\n",
    "                                      messages=messages,)\n",
    "\n",
    "# print(\"ChatCompletion (gpt-35-turbo) :\" + answer.choices[0].message.content)\n",
    "display(HTML(answer.choices[0].message.content))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **LangChain**\n",
    "\n",
    "LangChain is a framework built around Large Language Models (LLMs).\n",
    "\n",
    "The core idea of the library is that we can “chain” together different components to create more advanced use cases around LLMs.\n",
    "\n",
    "LangChain components: Models, Prompts, Indexes(Document Loaders and Splitters, Vector DBs), Chains and Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AzureOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.completion.Completion'>, model_name='gpt-35-turbo', temperature=0.0, max_tokens=400, top_p=1, frequency_penalty=0, presence_penalty=0, n=1, best_of=1, model_kwargs={'stop': ['<|im_end|>']}, openai_api_key='6ce2f2fae6e54aa0a8d670505917e3ee', openai_api_base='', openai_organization='', openai_proxy='', batch_size=20, request_timeout=None, logit_bias={}, max_retries=6, streaming=False, allowed_special=set(), disallowed_special='all', tiktoken_model_name=None, deployment_name='gpt-35-turbo', openai_api_type='azure', openai_api_version='2023-03-15-preview')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_llm(model=OPENAI_MODEL_NAME,\n",
    "             deployment_name=OPENAI_DEPLOYMENT_NAME,\n",
    "             openai_api_version=OPENAI_DEPLOYMENT_VERSION,\n",
    "             temperature=0,\n",
    "             max_tokens=400,\n",
    "             stop=\"<|im_end|>\",\n",
    "             ):\n",
    "\n",
    "    llm = AzureOpenAI(deployment_name=deployment_name,\n",
    "                      model=model,\n",
    "                      openai_api_version=openai_api_version,\n",
    "                      temperature=temperature,\n",
    "                      max_tokens=max_tokens,\n",
    "                      model_kwargs={\"stop\": [\"<|im_end|>\"]})\n",
    "    return llm\n",
    "\n",
    "# init LLM Azure OpenAI model\n",
    "llm = init_llm()\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "gpt-35-turbo:  I'm doing well, thank you.  I'm calling to follow up on the status of my application.  I submitted it on [date] and was wondering if there was any update on the hiring process.  Thank you for your time.\" \n",
       "\n",
       "If they ask for more information or want to schedule an interview, be prepared to answer questions about your qualifications and availability. Good luck!"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model \"gpt-35-turbo\"\n",
    "# You can see that gpt-35-turbo has been trained in QnA conversational style.\n",
    "answer = llm(\"Good morning, how are you? \")\n",
    "display(HTML(\"gpt-35-turbo: \" + answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "gpt-35-turbo:  Use a for loop, not the built-in reverse() function.\n",
       "\n",
       "def reverse_string(string):\n",
       "    reversed_string = \"\"\n",
       "    for i in range(len(string)-1, -1, -1):\n",
       "        reversed_string += string[i]\n",
       "    return reversed_string\n",
       "\n",
       "print(reverse_string(\"hello\")) # \"olleh\"\n",
       "print(reverse_string(\"racecar\")) # \"racecar\"\n",
       "print(reverse_string(\"python\")) # \"nohtyp\"<|im_sep|>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm = init_llm()\n",
    "answer = llm(\"Create a Python function that takes a string argument and reverses it.\")\n",
    "display(Markdown(\"gpt-35-turbo: \" + answer))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompts with LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "gpt-35-turbo:  To assess the risk tolerance of a new client, you can use a risk tolerance questionnaire. This questionnaire will ask the client a series of questions about their investment goals, investment experience, and risk preferences. Based on the answers provided, you can determine the client's risk tolerance and recommend an investment strategy that aligns with their risk tolerance."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "# create template for prompt\n",
    "\n",
    "template = \"\"\"You are a {profession} answering users questions. \n",
    "            More specifically, you are an expert in {expertise}. Answer in a clear and concise manner. Assume that the user is not a subject expert.\n",
    "            If a question is not clear or not related to {expertise} say: it's not clear or the question is not related to {expertise}.\n",
    "            \n",
    "            USER: {question}\n",
    "            ASSISTANT:\n",
    "            \n",
    "            <|im_end|>\n",
    "            \"\"\"\n",
    "\n",
    "llm = init_llm()\n",
    "prompt_template = PromptTemplate(template=template, input_variables=[\n",
    "                                 \"profession\", \"expertise\", \"question\"])\n",
    "answer = llm(prompt_template.format(profession=\"Financial Trading Consultant\",  expertise=\"Risk Management\",\n",
    "                                    question=\"How do you assess the risk tolerance of a new client?\"))\n",
    "display(Markdown(\"gpt-35-turbo: \" + answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "gpt-35-turbo:  It's not clear or the question is not related to Risk Management."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# asking not related question\n",
    "prompt_template = PromptTemplate(template=template, input_variables=[\n",
    "                                 \"profession\", \"expertise\", \"question\"])\n",
    "answer = llm(prompt_template.format(profession=\"Financial Trading Consultant\",  expertise=\"Risk Management\",\n",
    "                                    question=\"What's the fastest car in the world? Answer in one sentence. \"))\n",
    "display(Markdown(\"gpt-35-turbo: \" + answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ChatPromptTemplate is a template that is specifically designed for conversational use cases.\n",
    "\n",
    "It infers the variables from the template, so no need to pass them in as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are a {profession} answering users questions. \n",
    "            More specifically, you are an expert in {expertise}. Answer in a clear and concise manner. Assume that the user is not a subject expert.\n",
    "            If a question is not clear or not related to {expertise} say: it's not clear or the question is not related to {expertise}.\n",
    "            \n",
    "            USER: {question}\n",
    "            ASSISTANT:\n",
    "            \n",
    "            <|im_end|>\n",
    "            \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "gpt-35-turbo:  It's not clear or the question is not related to Risk Management."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ChatPromptTemplate figures out templates variables automatically\n",
    "answer = llm(prompt_template.format(profession=\"Financial Trading Consultant\",  expertise=\"Risk Management\",\n",
    "                                    question=\"What's the fastest car in the world? Answer in one sentence.\"))\n",
    "display(Markdown(\"gpt-35-turbo: \" + answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **OPTIONAL.** LLM output parsers with LangChain\n",
    "You can define the output schema and LangChain will parse the output for you.\n",
    "\n",
    "The main idea is to define parsing instructions as a code rather than doing it in textual form.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "customer_call = \"\"\"\n",
    "\n",
    "**Customer**: Hello, my name is John, and I'm a customer of Imaginal Bank.\n",
    "**Clerk**: Hello, John! My name is Sara, and I'm a customer service representative at Imaginal Bank. How can I assist you today?\n",
    "**Customer**: Hi, Sara. I'm interested in your bank's investment programs. \n",
    "              Can you tell me more about them, especially in terms of risk management?\n",
    "\n",
    "**Clerk**: Absolutely, John. We have a few key programs I can highlight.\n",
    "\n",
    "First, there's our 'Balanced Growth Fund'. It's a diversified mutual fund that invests in a mix of equities and bonds to provide both growth and income, reducing risk through diversification. \n",
    "\n",
    "We also have the 'Index Tracker ETF', which is designed to replicate the performance of a specific market index. By spreading investments across the entire index, it inherently reduces the risk associated with individual stocks.\n",
    "\n",
    "Additionally, for those with a lower risk tolerance, we have the 'Secure Income Bond Fund', which focuses on government and high-quality corporate bonds. \n",
    "\n",
    "Our financial advisors are always available to guide you in choosing the right program based on your financial goals and risk tolerance.\n",
    "\n",
    "**Customer**: I see. Could you elaborate on how the Balanced Growth Fund manages risk?\n",
    "\n",
    "**Clerk**: Sure. The Balanced Growth Fund mitigates risk by diversifying investments across a wide range of assets. If one investment performs poorly, it's likely to be offset by other investments that are performing well. Furthermore, our portfolio managers actively manage the fund, adjusting holdings based on changing market conditions to manage risk and enhance returns.\n",
    "\n",
    "**Customer**: Does the bank provide any tools to monitor my investments?\n",
    "\n",
    "**Clerk**: Yes, John. We offer an online platform called 'Imaginal Investor Dashboard'. It provides real-time tracking of your investments, balance updates, and market trends. You can also set up alerts to be notified about significant changes in your portfolio.\n",
    "\n",
    "**Customer**: That sounds quite comprehensive. How can I get started?\n",
    "\n",
    "**Clerk**: You can schedule an appointment with one of our financial advisors. They'll walk you through your options, help you understand your risk tolerance, and guide you in choosing the right investment program. Would you like me to arrange that for you?\n",
    "\n",
    "**Customer**: Yes, please. That would be helpful.\n",
    "\n",
    "**Clerk**: Fantastic, John! Let's get that set up for you...\"\"\"\n",
    "\n",
    "\n",
    "call_center_prompt_template = \"\"\"\n",
    "\n",
    "For the following text, extract the following information:\n",
    "agent politeness: How polite is the agent? use the following values to descibe agent politenes: very polite, polite, neutral, impolite, very impolite.\n",
    "agent knowledge: How knowledgeable is the agent? use the following values to descibe agent knowledge: very knowledgeable, knowledgeable, neutral, not knowledgeable, very not knowledgeable.\n",
    "customer issue resolution: How well did the agent resolve the issue? use the following values to descibe issue resolution: very well, well, neutral, not well, very not well.\n",
    "customer satisfaction: How satisfied is the customer? use the following values to descibe customer satisfaction: very satisfied, satisfied, neutral, dissatisfied, very dissatisfied.\n",
    "\n",
    "Format the output as a json object with the following keys: agent_politeness, agent_knowledge, customer_issue_resolution, customer_satisfaction.\n",
    "\n",
    "text: {text}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Output:\n",
      "\n",
      "{\n",
      "    \"agent_politeness\": \"very polite\",\n",
      "    \"agent_knowledge\": \"knowledgeable\",\n",
      "    \"customer_issue_resolution\": \"well\",\n",
      "    \"customer_satisfaction\": \"very satisfied\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "prompt_template = ChatPromptTemplate.from_template(call_center_prompt_template)\n",
    "# print (prompt_template)\n",
    "llm = init_llm()\n",
    "# Note that the type of the response is a string, it looks like a json object, but it's string\n",
    "response = llm(prompt_template.format(text=customer_call))\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"agent_politeness\": string  // How polite is the agent?\n",
      "\t\"agent_knowledge\": string  // How knowledgeable is the agent?\n",
      "\t\"customer_issue_resolution\": string  // How well did the agent resolve the issue?\n",
      "\t\"customer_satisfaction\": string  // How satisfied is the customer?\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "\n",
    "agent_politeness_schema = ResponseSchema(\n",
    "    name=\"agent_politeness\", description=\"How polite is the agent?\")\n",
    "aggent_knowledge_schema = ResponseSchema(\n",
    "    name=\"agent_knowledge\", description=\"How knowledgeable is the agent?\")\n",
    "customer_issue_resolution_schema = ResponseSchema(\n",
    "    name=\"customer_issue_resolution\", description=\"How well did the agent resolve the issue?\")\n",
    "customer_satisfaction_schema = ResponseSchema(\n",
    "    name=\"customer_satisfaction\", description=\"How satisfied is the customer?\")\n",
    "customer_service_schemas = [agent_politeness_schema, aggent_knowledge_schema,\n",
    "                            customer_issue_resolution_schema, customer_satisfaction_schema]\n",
    "\n",
    "output_parser = StructuredOutputParser.from_response_schemas(\n",
    "    customer_service_schemas)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "call_center_prompt_template_with_output_parser = \"\"\"\n",
    "\n",
    "For the following text, extract the following information:\n",
    "agent politeness: How polite is the agent? use the following values to descibe agent politenes: very polite, polite, neutral, impolite, very impolite.\n",
    "agent knowledge: How knowledgeable is the agent? use the following values to descibe agent knowledge: very knowledgeable, knowledgeable, neutral, not knowledgeable, very not knowledgeable.\n",
    "customer issue resolution: How well did the agent resolve the issue? use the following values to descibe issue resolution: very well, well, neutral, not well, very not well.\n",
    "customer satisfaction: How satisfied is the customer? use the following values to descibe customer satisfaction: very satisfied, satisfied, neutral, dissatisfied, very dissatisfied.\n",
    "\n",
    "Format the output as a json object with the following keys: agent_politeness, agent_knowledge, customer_issue_resolution, customer_satisfaction.\n",
    "\n",
    "text: {text}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    template=call_center_prompt_template_with_output_parser)\n",
    "input = prompt.format(text=customer_call,\n",
    "                      format_instructions=format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: \n",
      "\n",
      "For the following text, extract the following information:\n",
      "agent politeness: How polite is the agent? use the following values to descibe agent politenes: very polite, polite, neutral, impolite, very impolite.\n",
      "agent knowledge: How knowledgeable is the agent? use the following values to descibe agent knowledge: very knowledgeable, knowledgeable, neutral, not knowledgeable, very not knowledgeable.\n",
      "customer issue resolution: How well did the agent resolve the issue? use the following values to descibe issue resolution: very well, well, neutral, not well, very not well.\n",
      "customer satisfaction: How satisfied is the customer? use the following values to descibe customer satisfaction: very satisfied, satisfied, neutral, dissatisfied, very dissatisfied.\n",
      "\n",
      "Format the output as a json object with the following keys: agent_politeness, agent_knowledge, customer_issue_resolution, customer_satisfaction.\n",
      "\n",
      "text: \n",
      "\n",
      "**Customer**: Hello, my name is John, and I'm a customer of Imaginal Bank.\n",
      "**Clerk**: Hello, John! My name is Sara, and I'm a customer service representative at Imaginal Bank. How can I assist you today?\n",
      "**Customer**: Hi, Sara. I'm interested in your bank's investment programs. \n",
      "              Can you tell me more about them, especially in terms of risk management?\n",
      "\n",
      "**Clerk**: Absolutely, John. We have a few key programs I can highlight.\n",
      "\n",
      "First, there's our 'Balanced Growth Fund'. It's a diversified mutual fund that invests in a mix of equities and bonds to provide both growth and income, reducing risk through diversification. \n",
      "\n",
      "We also have the 'Index Tracker ETF', which is designed to replicate the performance of a specific market index. By spreading investments across the entire index, it inherently reduces the risk associated with individual stocks.\n",
      "\n",
      "Additionally, for those with a lower risk tolerance, we have the 'Secure Income Bond Fund', which focuses on government and high-quality corporate bonds. \n",
      "\n",
      "Our financial advisors are always available to guide you in choosing the right program based on your financial goals and risk tolerance.\n",
      "\n",
      "**Customer**: I see. Could you elaborate on how the Balanced Growth Fund manages risk?\n",
      "\n",
      "**Clerk**: Sure. The Balanced Growth Fund mitigates risk by diversifying investments across a wide range of assets. If one investment performs poorly, it's likely to be offset by other investments that are performing well. Furthermore, our portfolio managers actively manage the fund, adjusting holdings based on changing market conditions to manage risk and enhance returns.\n",
      "\n",
      "**Customer**: Does the bank provide any tools to monitor my investments?\n",
      "\n",
      "**Clerk**: Yes, John. We offer an online platform called 'Imaginal Investor Dashboard'. It provides real-time tracking of your investments, balance updates, and market trends. You can also set up alerts to be notified about significant changes in your portfolio.\n",
      "\n",
      "**Customer**: That sounds quite comprehensive. How can I get started?\n",
      "\n",
      "**Clerk**: You can schedule an appointment with one of our financial advisors. They'll walk you through your options, help you understand your risk tolerance, and guide you in choosing the right investment program. Would you like me to arrange that for you?\n",
      "\n",
      "**Customer**: Yes, please. That would be helpful.\n",
      "\n",
      "**Clerk**: Fantastic, John! Let's get that set up for you...\n",
      "\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"agent_politeness\": string  // How polite is the agent?\n",
      "\t\"agent_knowledge\": string  // How knowledgeable is the agent?\n",
      "\t\"customer_issue_resolution\": string  // How well did the agent resolve the issue?\n",
      "\t\"customer_satisfaction\": string  // How satisfied is the customer?\n",
      "}\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'polite'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict = output_parser.parse(response)\n",
    "dict.get(\"agent_politeness\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompts management best practices\n",
    "Reuse prompts as much as possible. This will help you to get more consistent results.\n",
    "\n",
    "Treat your prompts as a code, keep it in a version control system. This will help you to track changes and to revert them if needed.\n",
    "\n",
    "Use LangChain specific to use case prompt templates, e.g. ChatPromptTemplate for conversational flows.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  **One-shot, Few-shot learning**\n",
    "\n",
    "This technique could improve model performance by a lot. \n",
    "We can use the model to learn from a few examples and then use it to generate text. This is called few-shot learning. We can also use the model to learn from a single example and then use it to generate text. This is called one-shot learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_few_shot = \"\"\"You are a {profession} answering users questions. \n",
    "            More specifically, you are an expert in {expertise}. Answer in a clear and concise manner. Assume that a user is not a subject expert.\n",
    "            If a question is not clear or not related to {expertise} say: it's not clear or the question is not related to {expertise}.\n",
    "           \n",
    "            USER: How do you assess the risk tolerance of a new client?\n",
    "            ASSISTANT: I begin by having a comprehensive discussion with the client about their financial goals, investments horizon, and comfort level with different levels of risk.\n",
    "            \n",
    "            USER: Can you provide an example of a specific risk management strategy you'd recommended to a client in a volatile market situation?\n",
    "            ASSISTANT: During the market volatility caused by the pandemic, I'd recommended that a client diversify their portfolio further to reduce risk exposure.\n",
    "            \n",
    "            USER: How do you handle the situation when a client wants to pursue a risky investment that goes beyond their risk tolerance?\n",
    "            ASSISTANT: I would clearly communicate the potential risks associated with the investment and how it might not align with their established risk tolerance. \n",
    "            \n",
    "            USER: {question}\n",
    "            ASSISTANT:\n",
    "            \n",
    "            <|im_end|>\n",
    "            \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " There are many tools available to assist in risk management, including portfolio management software, risk assessment tools, and financial modeling software. I use these tools to help clients understand the potential risks associated with different investments and to develop strategies to mitigate those risks. Additionally, I use technology to monitor market trends and to keep clients informed about changes that may impact their investments."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt_few_shot = PromptTemplate(template=template_few_shot, input_variables=[\n",
    "                                 \"profession\", \"expertise\", \"question\"])\n",
    "chain = LLMChain(llm=llm, prompt=prompt_few_shot)\n",
    "\n",
    "res = chain.run(profession=\"Financial Trading Consultant\",  expertise=\"Risk Management\",\n",
    "                question=\"How do you use technology or specific financial tools to assist in risk management for your clients?\")\n",
    "display(Markdown(res))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Large Language Models (LLMs) are stateless. This means that they don’t retain any information about the conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " It's not clear or the question is not related to Risk Management."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Since we don't save a history of the conversation, the model will fail to answer questions that require context.\n",
    "res = chain.run(profession=\"Financial Trading Consultant\",  expertise=\"Risk Management\",\n",
    "                question=\"Which software do you use?\")\n",
    "display(Markdown(res))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Retain conversation history** "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The Large Language Models (LLMs) are stateless. This means that they don’t retain any information about the conversation history.\n",
    "Each transaction is independent of the previous one. Chatbots keep in memory the conversation history and use it to generate the next response. This is why they are able to generate more coherent responses.\n",
    "\n",
    "Previously we saw that a model fails to answer the question that requires context. We can solve this problem by retaining the conversation history. We can do this by using the LangChain ConversationBufferMemory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are a {profession} answering users questions. \n",
    "            More specifically, you are an expert in {expertise}. Answer in a clear and concise manner. Assume that the user is not a subject expert.\n",
    "            If a question is not clear or not related to {expertise} say: it's not clear or the question is not related to {expertise}.\n",
    "            \n",
    "            USER: {question}\n",
    "            ASSISTANT:\n",
    "\n",
    "            <|im_end|>\n",
    "            \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = init_llm()\n",
    "\n",
    "# ConversationBufferMemory is a memory that stores the conversation history\n",
    "memory = ConversationBufferMemory()\n",
    "# try to change the verbose to True, to see more details\n",
    "conversation = ConversationChain(llm=llm, memory=memory, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " Hello! I'm happy to help. There are many ways that technology and financial tools can be used to assist in risk management. One of the most common ways is through the use of risk management software. This software can help identify potential risks and provide tools to mitigate those risks. Additionally, there are many financial tools that can be used to help manage risk, such as options, futures, and other derivatives. These tools can be used to hedge against potential losses and protect against market volatility. Finally, there are many data analysis tools that can be used to help identify trends and patterns in financial data, which can be used to make more informed decisions about risk management. Does that help?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_template(template=template)\n",
    "\n",
    "response = conversation.run(input=prompt.format(profession=\"Financial Trading Consultant\",\n",
    "                                                expertise=\"Risk Management\",\n",
    "                                                question=\"How do you use technology or specific financial tools to assist in risk management for your clients?\"))\n",
    "\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "  There are many different risk management software products available, and the specific software that we use depends on the needs of our clients. Some of the most popular risk management software products include:\n",
       "\n",
       "- RiskMetrics\n",
       "- MSCI RiskManager\n",
       "- Algorithmics RiskWatch\n",
       "- SAS Risk Management\n",
       "- Bloomberg Risk Management\n",
       "\n",
       "Each of these products has its own strengths and weaknesses, and the best choice depends on the specific needs of the client. Does that answer your question?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now with the conversation history, the model can answer questions that require context.\n",
    "response = conversation.run(input=prompt.format(profession=\"Financial Trading Consultant\",\n",
    "                                                expertise=\"Risk Management\",\n",
    "                            question=\"Which software do you use? List software products in a separate line.\"))\n",
    "\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human: Human: You are a Financial Trading Consultant answering users questions. \n",
      "            More specifically, you are an expert in Risk Management. Answer in a clear and concise manner. Assume that the user is not a subject expert.\n",
      "            If a question is not clear or not related to Risk Management say: it's not clear or the question is not related to Risk Management.\n",
      "            \n",
      "            USER: How do you use technology or specific financial tools to assist in risk management for your clients?\n",
      "            ASSISTANT:\n",
      "\n",
      "            <|im_end|>\n",
      "            \n",
      "ai:  Hello! I'm happy to help. There are many ways that technology and financial tools can be used to assist in risk management. One of the most common ways is through the use of risk management software. This software can help identify potential risks and provide tools to mitigate those risks. Additionally, there are many financial tools that can be used to help manage risk, such as options, futures, and other derivatives. These tools can be used to hedge against potential losses and protect against market volatility. Finally, there are many data analysis tools that can be used to help identify trends and patterns in financial data, which can be used to make more informed decisions about risk management. Does that help?\n",
      "human: Human: You are a Financial Trading Consultant answering users questions. \n",
      "            More specifically, you are an expert in Risk Management. Answer in a clear and concise manner. Assume that the user is not a subject expert.\n",
      "            If a question is not clear or not related to Risk Management say: it's not clear or the question is not related to Risk Management.\n",
      "            \n",
      "            USER: Which software do you use? List software products in a separate line.\n",
      "            ASSISTANT:\n",
      "\n",
      "            <|im_end|>\n",
      "            \n",
      "ai:   There are many different risk management software products available, and the specific software that we use depends on the needs of our clients. Some of the most popular risk management software products include:\n",
      "\n",
      "- RiskMetrics\n",
      "- MSCI RiskManager\n",
      "- Algorithmics RiskWatch\n",
      "- SAS Risk Management\n",
      "- Bloomberg Risk Management\n",
      "\n",
      "Each of these products has its own strengths and weaknesses, and the best choice depends on the specific needs of the client. Does that answer your question?\n"
     ]
    }
   ],
   "source": [
    "# you can print the conversation history buffer:\n",
    "history = conversation.memory.chat_memory.messages\n",
    "for msg in history:\n",
    "    print(f\"{msg.type}: {msg.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['history', 'input'] output_parser=None partial_variables={} template='The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\n\\nCurrent conversation:\\n{history}\\nHuman: {input}\\nAI:' template_format='f-string' validate_template=True\n",
      "\n",
      "{'history': \"Human: Human: You are a Financial Trading Consultant answering users questions. \\n            More specifically, you are an expert in Risk Management. Answer in a clear and concise manner. Assume that the user is not a subject expert.\\n            If a question is not clear or not related to Risk Management say: it's not clear or the question is not related to Risk Management.\\n            \\n            USER: How do you use technology or specific financial tools to assist in risk management for your clients?\\n            ASSISTANT:\\n\\n            <|im_end|>\\n            \\nAI:  Hello! I'm happy to help. There are many ways that technology and financial tools can be used to assist in risk management. One of the most common ways is through the use of risk management software. This software can help identify potential risks and provide tools to mitigate those risks. Additionally, there are many financial tools that can be used to help manage risk, such as options, futures, and other derivatives. These tools can be used to hedge against potential losses and protect against market volatility. Finally, there are many data analysis tools that can be used to help identify trends and patterns in financial data, which can be used to make more informed decisions about risk management. Does that help?\\nHuman: Human: You are a Financial Trading Consultant answering users questions. \\n            More specifically, you are an expert in Risk Management. Answer in a clear and concise manner. Assume that the user is not a subject expert.\\n            If a question is not clear or not related to Risk Management say: it's not clear or the question is not related to Risk Management.\\n            \\n            USER: Which software do you use? List software products in a separate line.\\n            ASSISTANT:\\n\\n            <|im_end|>\\n            \\nAI:   There are many different risk management software products available, and the specific software that we use depends on the needs of our clients. Some of the most popular risk management software products include:\\n\\n- RiskMetrics\\n- MSCI RiskManager\\n- Algorithmics RiskWatch\\n- SAS Risk Management\\n- Bloomberg Risk Management\\n\\nEach of these products has its own strengths and weaknesses, and the best choice depends on the specific needs of the client. Does that answer your question?\"}\n"
     ]
    }
   ],
   "source": [
    "# pay attention that LangChain added to the prompt: ```Current conversation:{history}```\n",
    "print(conversation.prompt)\n",
    "print()\n",
    "print(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "  Sure, here are all the questions you've asked me about risk management:\n",
       "\n",
       "- How do you use technology or specific financial tools to assist in risk management for your clients?\n",
       "- Which software do you use? List software products in a separate line.\n",
       "\n",
       "Is there anything else I can help you with?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ans = conversation.run(input=prompt.format(profession=\"Financial Trading Consultant\",\n",
    "                                           expertise=\"Risk Management\",\n",
    "                                           question=\"List all questions I've asked you about Risk Management?\"))\n",
    "display(Markdown(ans))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More conversation memory types.\n",
    "\n",
    "Keeping full conversation history could be expensive and we can hit the Azure API limits. LangChain provides different types of conversation memory that can be used to keep the conversation history and mitigate the limits issues.\n",
    "\n",
    "**1. ConversationBufferWindowMemory** - keeps the last N messages in the conversation history.\n",
    "\n",
    "**2. ConversationTokenBufferMemory** - keeps the last N tokens in the conversation history.\n",
    "\n",
    "**3. ConversationSummaryBufferMemory** - keeps summary of the conversation over time.\n",
    "\n",
    "Additional Memory Type includes:\n",
    "\n",
    "**4. Vector data memory** - stires the text in a vector DB and retrieves most semantically similar text.\n",
    "\n",
    "**5. Entity memories** - use LLM which rememebers details about specific entities. \n",
    "\n",
    "Note: You can use multiple memories at the same time. \n",
    "\n",
    "Finally you can store the conevrsation history in a conventional database like SQL or NoSQL. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: What's your name\\nAI: My name is John\"}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "memory.save_context({\"input\": \"hi\"}, {\"output\": \"hello\"})\n",
    "memory.save_context({\"input\": \"What's your name\"},\n",
    "                    {\"output\": \"My name is John\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
